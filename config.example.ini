# =========================================================================
# Anomaly Detection Engine Configuration
# =========================================================================
# This file controls all operational parameters of the engine.
# Settings can be reloaded live by sending a SIGHUP signal to the process.

# --- Core I/O and General Settings ---
# The type of log source to read from. Supported types: "file", "mongodb"
log_source_type = mongodb
# Where to read logs from. Use "stdin" to process from standard input.
log_input_path = ./data/fake.log
# Path to a file to save the current state of the log reader.
reader_state_path = data/reader_state.dat
# Path to a file containing one IP address or CIDR range per line to ignore.
allowlist_path = ./data/allowlist.txt
# If true, prints alerts in a human-readable format to the console.
alerts_to_stdout = true
# If true, writes alerts in JSON format to the file specified below.
alerts_to_file = true
alert_output_path = ./data/alerts_log.json


# --- Alert Throttling ---
# To prevent alert fatigue, the engine can suppress duplicate alerts.
# An alert is a "duplicate" if it has the same source IP and reason.
# Time (in seconds) to suppress a duplicate alert after it's first seen.
alert_throttle_duration_seconds = 300
# A duplicate alert WILL be shown if this many other unique alerts have
# occurred since, even if the time window hasn't passed. (0 = no limit)
alert_throttle_max_alerts = 10


# --- State Management ---
# The engine can save its learned baselines to a file to survive restarts.
state_persistence_enabled = true
state_file_path = data/engine_state.dat
state_save_interval_events = 50000
# If true, the engine will periodically remove old, inactive state objects.
state_pruning_enabled = true
# Time (in seconds) an IP or Path must be inactive before being pruned. (7 days)
state_ttl_seconds = 604800
state_prune_interval_events = 100000


# --- Live File Monitoring ---
# If true and `log_input_path` is a file, the engine will "tail" the file
# for new lines after reaching the end, instead of exiting.
live_monitoring_enabled = false
live_monitoring_sleep_seconds = 0.1


# --- Machine Learning Data Collection ---
# For development: if true, all generated feature vectors will be saved
# to a CSV file, perfect for training a new model.
ml_data_collection_enabled = false
ml_data_collection_path = data/training_features.csv


# =========================================================================
# Tier 1: Heuristic & Rule-Based Detection
# Fast, simple checks for obvious threats.
# =========================================================================
[Tier1]
enabled = true

# --- Window-Based Rules ---
# The duration (in seconds) for short-term checks like request rates.
sliding_window_duration_seconds = 60
max_requests_per_ip_in_window = 1000
max_failed_logins_per_ip = 50
failed_login_status_codes = 401,403

# --- User-Agent Anomaly Detection ---
check_user_agent_anomalies = true
min_chrome_version = 90
min_firefox_version = 85
max_unique_uas_per_ip_in_window = 3

# --- String/Pattern Matching ---
# Comma-separated lists of substrings to search for.
suspicious_path_substrings = ../,/etc/passwd,sqlmap,xss,script
suspicious_ua_substrings = Nmap,sqlmap,nikto,curl/
# Paths that are sensitive, especially when accessed by a newly seen IP.
sensitive_path_substrings = admin,config,backup,.env,login,wp-admin,wp-login
# Limit the number of unique paths stored per IP to prevent memory exhaustion. (0 = no limit)
max_unique_paths_stored_per_ip = 2000

# --- Session Tracking ---
# A "session" is a series of requests from a unique combination of components.
session_tracking_enabled = true
# Defines a unique session. Can be any combination of "ip", "ua".
session_key_components = ip,ua
session_inactivity_ttl_seconds = 1800
max_failed_logins_per_session = 10
max_requests_per_session_in_window = 30
max_ua_changes_per_session = 2

# --- Content-Type Ratio Rules (for detecting scraping) ---
# Defines which requests are considered "HTML pages" vs "assets".
html_path_suffixes = .html,.htm,.php,.jsp
html_exact_paths = /
asset_path_prefixes = /static/,/assets/,/public/,/media/,/api/
asset_path_suffixes = .css,.js,.png,.jpg,.jpeg,.gif,.svg,.woff2,.ico
# How many HTML pages an IP must request before the ratio check is performed.
min_html_requests_for_ratio_check = 5
# Alert if (assets / html_pages) is below this ratio. Bots often don't fetch assets.
min_assets_per_html_ratio = 10.0


# =========================================================================
# Tier 2: Statistical Anomaly Detection
# Identifies deviations from learned baselines.
# =========================================================================
[Tier2]
enabled = true
# An event's Z-score must be above this to be considered anomalous.
z_score_threshold = 3.5
# How many data points are needed before Z-scores are calculated for an IP/path.
min_samples_for_z_score = 30
# A simpler check: alert if a value is this many times its historical average.
historical_deviation_factor = 3.0


# =========================================================================
# Tier 3: Machine Learning Detection
# Finds complex, multi-variate anomalies.
# =========================================================================
[Tier3]
enabled = false
# Path to the ONNX model file for inference.
model_path = src/models/isolation_forest.onnx
# The model's output score must be above this threshold to trigger an alert.
anomaly_score_threshold = 0.6
# Path to the JSON file containing model metadata (e.g., feature names).
model_metadata_path = src/models/isolation_forest.json
# If true, the engine will periodically run the training script and hot-swap the model.
automated_retraining_enabled = false
retraining_interval_seconds = 86400
training_script_path = ml/train.py


# =========================================================================
# Alerting: Where to send alerts.
# =========================================================================
[Alerting]
file_enabled = true
syslog_enabled = true
http_enabled = false
# The full URL for the HTTP webhook (e.g., for Slack, a SIEM, or a custom API).
; http_webhook_url = https://your-siem-or-slack-webhook.com/alerts


# =========================================================================
# Threat Intelligence: External Blacklists
# =========================================================================
[ThreatIntel]
enabled = true
# How often (in seconds) to re-download the feeds. (1 hour)
update_interval_seconds = 3600
# Comma-separated list of URLs pointing to plaintext IP blocklists.
feed_urls = https://raw.githubusercontent.com/firehol/blocklist-ipsets/refs/heads/master/firehol_level1.netset


[MongoLogSource]
# Settings used only if log_source_type is "mongodb"
uri = mongodb://localhost:27017
database = database
collection = collection
# The BSON field name in the log collection that contains the event timestamp.
# The engine will query based on this field.
timestamp_field_name = ts

[Logging]
# 1. Set a "catch-all" default level for any component not specified.
#    Let's make it INFO so we see important messages but not noise.
default_level = INFO

# 2. Let's say we don't care about I/O details at all. We can silence
#    the entire IO category using a wildcard.
io.* = WARN

# 3. We are debugging a very specific Z-score calculation. We want to
#    see every detail about it, but not other analysis noise.
#    We set the parent `analysis` category to DEBUG, but the specific
#    `analysis.zscore` to the most verbose level, TRACE.
analysis.* = DEBUG
analysis.zscore = TRACE

# 4. We also want to see which rules are being evaluated and when they fire,
#    but we don't need the most verbose TRACE level for that.
rules.eval = DEBUG
rules.t1 = DEBUG

[Monitoring]
# Enable fine-grained, function-level performance timers.
# This adds some overhead and should only be used for debugging/profiling.
enable_deep_timing = false
# The host IP for the monitoring web server to bind to.
web_server_host = 0.0.0.0
# The port for the monitoring web server.
web_server_port = 9090

# =========================================================================
# Prometheus Integration: Metrics Export and Monitoring
# =========================================================================
[Prometheus]
# Enable Prometheus metrics export
enabled = true
# Host and port for the Prometheus metrics endpoint
host = 0.0.0.0
port = 9090
# HTTP paths for metrics and health endpoints
metrics_path = /metrics
health_path = /health
# How often Prometheus should scrape metrics (in seconds)
scrape_interval_seconds = 15
# If true, disable the existing web server when Prometheus is enabled
replace_web_server = false
# Maximum age of metrics before they are considered stale (in seconds)
max_metrics_age_seconds = 300

# =========================================================================
# Dynamic Learning: Adaptive Threshold Management
# =========================================================================
[DynamicLearning]
# Enable dynamic learning and adaptive thresholds
enabled = true
# Time window for learning patterns (in hours)
learning_window_hours = 24
# Confidence threshold for accepting learned baselines (0.0-1.0)
confidence_threshold = 0.95
# Minimum number of samples required before learning begins
min_samples_for_learning = 100
# Sensitivity for seasonal pattern detection (0.0-1.0)
seasonal_detection_sensitivity = 0.8
# How often to update baselines (in seconds)
baseline_update_interval_seconds = 300
# Allow manual threshold overrides for critical security rules
enable_manual_overrides = true
# Maximum percentage change allowed for threshold adjustments
threshold_change_max_percent = 50.0

# Enhanced Adaptive Threshold Settings
# Enable percentile-based threshold calculations
enable_percentile_thresholds = true
# Default percentile for normal threshold calculations (0.5-1.0)
default_percentile_95 = 0.95
# Default percentile for strict threshold calculations (0.5-1.0)
default_percentile_99 = 0.99
# Cache threshold calculations for this many seconds (10-3600)
threshold_cache_ttl_seconds = 60
# Enable special handling for security-critical entities
enable_security_critical_entities = true
# Maximum percentage change allowed for security-critical entities (1.0-100.0)
security_critical_max_change_percent = 10.0
# Maximum number of audit entries to keep per entity (10-1000)
max_audit_entries_per_entity = 100
# Enable threshold change validation and rejection
enable_threshold_change_validation = true

# Automatic Security-Critical Entity Detection
# Automatically mark login-related paths as security critical
auto_mark_login_paths_critical = true
# Automatically mark admin-related paths as security critical
auto_mark_admin_paths_critical = true
# Automatically mark IPs with high failed login counts as security critical
auto_mark_high_failed_login_ips_critical = true
# Failed login count threshold for marking IPs as security critical (1-100)
failed_login_threshold_for_critical = 5

# =========================================================================
# Tier 4: Prometheus-Based Anomaly Detection
# =========================================================================
[Tier4]
# Enable Tier 4 Prometheus-based detection
enabled = false
# URL of the Prometheus server to query
prometheus_url = http://localhost:9090
# Timeout for Prometheus queries (in seconds)
query_timeout_seconds = 30
# How often to evaluate Tier 4 rules (in seconds)
evaluation_interval_seconds = 60
# Maximum number of concurrent Prometheus queries
max_concurrent_queries = 10
# Authentication token for secured Prometheus instances (optional)
auth_token = 
# Enable circuit breaker for Prometheus connectivity
enable_circuit_breaker = true
# Number of failures before circuit breaker opens
circuit_breaker_failure_threshold = 5
# Time to wait before attempting recovery (in seconds)
circuit_breaker_recovery_timeout_seconds = 60

# =========================================================================
# Memory Management: Resource Optimization and Limits
# =========================================================================
[MemoryManagement]
# Enable memory management and monitoring
enabled = true
# Maximum memory usage allowed (in MB)
max_memory_usage_mb = 1024
# Memory pressure threshold for triggering cleanup (in MB)
memory_pressure_threshold_mb = 800
# Enable object pooling for frequently allocated objects
enable_object_pooling = true
# How often to check for memory pressure and cleanup (in seconds)
eviction_check_interval_seconds = 60
# Memory usage percentage that triggers eviction
eviction_threshold_percent = 80.0
# Enable memory compaction during pressure situations
enable_memory_compaction = true
# Time-to-live for inactive state objects (in seconds)
state_object_ttl_seconds = 3600

# =========================================================================
# Performance Monitoring: System Performance and Load Shedding
# =========================================================================
[PerformanceMonitoring]
# Enable performance monitoring system
enabled = true
# Enable detailed function profiling (adds overhead)
enable_profiling = false
# Enable adaptive load shedding under high load
enable_load_shedding = true

# Metrics collection settings
metrics_collection_interval_ms = 1000
max_latency_samples_per_component = 10000

# Performance thresholds for load shedding decisions
max_cpu_usage_percent = 80.0
max_memory_usage_bytes = 1073741824  # 1GB
max_queue_depth = 10000
max_avg_latency_ms = 1000
max_error_rate_percent = 5.0

# Load shedding percentages for different load levels
moderate_load_shed_percentage = 10.0
high_load_shed_percentage = 25.0
critical_load_shed_percentage = 50.0

# Monitoring loop configuration
monitoring_loop_interval_seconds = 5

# Function profiling settings (when enabled)
enable_function_profiling = false
max_profile_samples_per_function = 1000
profile_report_interval_seconds = 300

# Performance reporting
enable_performance_reports = true
performance_report_path = "data/performance_report.csv"
performance_report_interval_seconds = 60

# =========================================================================
# Error Handling: Recovery and Resilience
# =========================================================================
[ErrorHandling]
# Enable comprehensive error handling system
enabled = true

# Circuit breaker configuration for external dependencies
enable_circuit_breaker = true
circuit_breaker_failure_threshold = 5
circuit_breaker_timeout_ms = 5000
circuit_breaker_recovery_timeout_ms = 30000

# Error recovery settings
enable_error_recovery = true
max_retry_attempts = 3
initial_retry_delay_ms = 100
max_retry_delay_ms = 5000
retry_backoff_multiplier = 2.0

# Graceful degradation thresholds
enable_graceful_degradation = true
cpu_threshold_for_degradation = 85.0
memory_threshold_for_degradation_mb = 900
queue_depth_threshold_for_degradation = 15000
error_rate_threshold_for_degradation = 10.0

# Component-specific recovery strategies
# Options: RETRY, CIRCUIT_BREAK, FALLBACK, FAIL_FAST
default_recovery_strategy = "RETRY"
prometheus_recovery_strategy = "CIRCUIT_BREAK"
database_recovery_strategy = "RETRY"
file_io_recovery_strategy = "FALLBACK"
network_recovery_strategy = "CIRCUIT_BREAK"

# Error rate limiting to prevent log spam
enable_error_rate_limiting = true
max_errors_per_minute = 100
error_burst_limit = 10

# Recovery monitoring and logging
recovery_statistics_interval_seconds = 60
log_recovery_attempts = true
recovery_log_level = "INFO"